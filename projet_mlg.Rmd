---
title: "Projet_MLG"
author: "Karim Assaad, Wassim Ben Youssef et Yoann Dacruz"
date: "26 novembre 2015"
output: pdf_document
toc: yes
---

```{r echo=FALSE}
load("/Users/wassim/Desktop/MLG/projet/hedonic.rda")
head(Hedonic)


test <- sample(c(0,1), 506, replace=TRUE)
head(test)
Hedonic <- cbind(Hedonic, test)
#table(test)
#str(Hedonic)
#idx <- which(Hedonic[,16]==1)
#Hedonic[idx,5]

Hedonic.train<-subset(Hedonic,Hedonic$test==1,-test)
Hedonic.test<-subset(Hedonic,test==0,-test)

```

#Premi√®re Partie:Description du jeu de donn√©e et analyse exploratoire

##Introduction
<p> Le projet trait√© ici propose d'appliquer des mod√©les de r√©gression et de r√©gularisation dans le but d'√©laborer un mod√©le pr√©dictif d'un jeu de donn√©es.</p>

<p> Le jeu de donn√©es "Hedonic" a ete choisi pour cette etude parmi les diff√©rents jeu de donn√©es propos√©s. Ce jeu de donn√©es recense le prix m√©dian des maisons pour des secteurs de la ville de Boston √©valu√© √† partir de plusieurs facteurs √©conomiques et sociaux en 1970.</p>

<p> Le jeu de donn√©es Hedonic" poss√©de ainsi 15 variables pour un total de 506 secteurs. La principale variable qu'il faut pr√©dire en fonction des autres variables est la variable "mv" qui est le prix m√©dian des maison d'un secteur. Parmi toutes ces variables, une est qualitative : la variable "chas" qui informe par oui ou par non si le secteur est situ√© dans les environs de la rivi√©re Charles. Les autres variables, toutes quantitatives sont :</p>
* "crim rate" qui informe du taux de criminalit√© du secteur
* "zn" qui est le nombre de lots r√©sidentiels d'une superficie de 25000 pieds carr√©s 
* "indus" qui est la proportion d'acres (une unit√© de mesure √©quivalente √† l'hectare) d'industries et de commerces interentreprises (inversement au commerce de d√©tail) 
* "nox" la concentration d'oxyde d'azote annuelle moyenne en pour centaine de millions 
* "rm" le nombre moyen de pi√©ces par maison 
* "age" la proportion de propri√©t√©s construites avant 1940 
* "dis" les distance pond√©r√©es √† cinq centre d'emplois dans la r√©gion de Boston 
* "rad" l'index d'accessibilit√© √† des voies de circulation importantes joignant le centre de l'agglom√©ration √† une voie p√©riph√©rique ou √† une ville de province 
* "tax" le taux de la valeur de l'imp√©t foncier en $/10000$ (dollar par 10000 dollars) 
* "pratio" le ratio d'√©l√©ves par professeurs 
* "blacks" la proportion de personnes noirs dans la population 
* "lstat" la proportion de la population ayant un statut social bas 
* "townid" l'identifiant de la ville 


##Description des donn√©es
```{r echo=FALSE}

#par(mfrow=c(1,1))
plot(ecdf(Hedonic.train$mv),main="Fonction de r√©partition", xlab="")
hist(Hedonic.train$mv,main="Histogramme",xlab="")
plot(density(Hedonic.train$mv),main="Estimation de densit√©",xlab="")
rug(Hedonic.train$mv)
title(outer=TRUE, main ="\nDistribution de la variable r√©ponse",sub="Pourcentage de votes nuls")

```

<p> D'apr√®s l'histogramme de la variable mv, nous pouvons voir qu'on a une forte concentration des valeurs de mv aux alentours de 10. Nous observons aussi une fr√©quence maximale de 135 pour des valeurs de mv entre 9.75 et 10. Nous arrivons aux m√™mes conclusion en effectuant l'analyse du trac√© de l'estimation de densit√© de cette variable.</p>


```{r echo=FALSE}
par(mfrow=c(1,1))
boxplot(Hedonic.train, main="Box-Plot des varaibles quantitative")
#boxplot(Hedonic.train$mv)
#boxplot(Hedonic.train$crim)
#boxplot(Hedonic.train$zn)
#boxplot(Hedonic.train$indus)
#boxplot(Hedonic.train$nox)
#boxplot(Hedonic.train$rm)
#boxplot(Hedonic.train$age)
#boxplot(Hedonic.train$dis)
#boxplot(Hedonic.train$rad)
#boxplot(Hedonic.train$tax)
#boxplot(Hedonic.train$ptratio)
#boxplot(Hedonic.train$blacks)
#boxplot(Hedonic.train$lstat)
#boxplot(Hedonic.train$townid)

```
<p>En tra√ßant les box-plots pour chacune des variables, nous remarquons que pour les variables mv, crim, zn, rm et blacks </p>


```{r echo=FALSE}
are.factor  <-sapply(Hedonic.train, is.factor)
#install.packages("GGally")
#library(GGally)

#install.packages(ggally)

#are.factor  <-sapply(Hedonic, is.factor)
#ggpairs(Hedonic, columns =which(!are.factor))

```

<p>Commentaire</p>

```{r echo=FALSE}
heatmap(abs(cor(Hedonic.train[, !are.factor])),main="Heatmap")


```

<p> En observant la matrice des corr√©lations entre les variables, nous concluons que 3 paires de variables sont corr√©l√©es : mv avec lstat, tax avec rad et dis avec nox. Cela signifie que nous pouvons expliquer une variable par l'autre. </p>


```{r echo=FALSE}
nb_manquantes<-sapply(Hedonic.train, function(x) sum(length(which(is.na(x))))) 
nb_manquantes

```
<p> D'apr√®s ce tableau, le jeu de donn√©es Hedonic ne contient pas de valeurs manquantes.</p>




#Deuxieme Partie:Description des m√©thodes appliqu√©es


##Mod√®le pr√©dictif et interpr√©table pour la variable mv
```{r echo=FALSE}
Hedonic.train$chas<-as.numeric(Hedonic.train$chas)
full<-lm(mv~., Hedonic.train)
summary(full)$coefficient
null<-lm(mv~1, Hedonic.train)
summary(null)$coefficient

#calcul de l'erreur
calcul_erreur<-full$coefficients[1]+full$coefficients[2]*Hedonic.train[2]+full$coefficients[3]*Hedonic.train[3]+full$coefficients[4]*Hedonic.train[4]+full$coefficients[6]*Hedonic.train[6]+full$coefficients[7]*Hedonic.train[7]+full$coefficients[8]*Hedonic.train[8]+full$coefficients[9]*Hedonic.train[9]+full$coefficients[10]*Hedonic.train[10]+full$coefficients[11]*Hedonic.train[11]+full$coefficients[12]*Hedonic.train[12]+full$coefficients[13]*Hedonic.train[13]+full$coefficients[14]*Hedonic.train[14]

calcul_erreur<-Hedonic.train[1]-calcul_erreur
calcul_erreur
erreur<-mean(calcul_erreur$mv)
erreur
```  
<p> Dans le summary de full nous pouvons voir que les estimations de chacun des coefficients est assez bonne puisque l'erreur standard la plus importante est celle du coefficient "blacks" et est de 0.147. Cependant, l'intercept est assez mal estim√© avec un mod√®le utilisant tout les coefficients, il est mieux estim√© dans "null" lorsqu'on ne prend aucune variable et l'on voit que son erreur standard est de 0.025. Nous pouvons penser au vu des erreurs standards obtenues que notre mod√®le est assez bon. </p>

<p> Pour faire une estimation de l'erreur, nous d√©cidons de soustraire √† la colonne des "mv" le mod√®le full obtenu, puis de faire la moyenne de cette colonne. Nous trouvons alors une erreur de 0.016, le mod√®le trouv√© pr√©cedemment a donc une erreur assez faible pour ce mod√®le full. </p>
  

```{r echo=FALSE}
library(leaps)
out <- regsubsets(mv ~ . , data=Hedonic.train,nbest=500, really.big=TRUE, nvmax=14)
bss <- summary(out)
bss.size <- as.numeric(rownames(bss$which))

intercept <- lm(mv ~ 1, data=Hedonic.train)
intercept

bss.best.rss <- c(sum(resid(intercept)^2), tapply(bss$rss, bss.size, min))
bss.best.rss

plot(0:14, bss.best.rss, ylim=c(0, 100), type="b",xlab="subset size", ylab="RSS", col="red2" )

points(bss.size, bss$rss, pch=20, col="gray", cex=0.7)
```
<p> Le graphe montre bien que plus nous prenons un nombre important de variables dans le mod√®le, plus l'estimation des r√©sidus est faible.   Donc le meilleur mod√®le est celui qui prend en compte toutes les variables.</p>


```{r echo=FALSE}
bss.best.cp <- c(sum(resid(intercept)^2), tapply(bss$cp, bss.size, min))
bss.best.cp

plot(0:14, bss.best.cp, ylim=c(0, 400), type="b",xlab="subset size", ylab="CP", col="red2" )
points(bss.size, bss$cp, pch=20, col="gray", cex=0.7)
```

<p> Le graphe montre que le coefficient Cp diminue plus le nombre de variables prises pour le mod√®le est important et ceci jusqu'√† la dixi√®me variable, ce qui signifie que plus l'on prend de variables, plus le mod√®le est bon jusqu'√† la dixi√®me variable. A partir de 10 variables, on observe cependant une l√©g√®re augmentation du coefficient Cp, et ceci jusqu'√† la quatorzi√®me variable, mais pour le mod√®le √† 14 variables, il n'y a qu'un seul mod√®le. Donc le mod√®le √† 14 variables semble √©galement bon.</p>

```{r echo=FALSE}

lower <- ~1
upper <- ~mv~crim+zn+indus+nox+rm+age+dis+rad+tax+ptratio+blacks+lstat+townid
scope <- list(lower=lower,upper=upper)

#AIC.fwd <- step(null, scope, direction="forward" , trace=FALSE)
#AIC.fwd
#AIC.bwd <- step(full, scope, direction="backward", trace=FALSE)
#AIC.bwd
step.AIC <-step(lm(mv~.,Hedonic.train),k=2)

step.BIC <-step(lm(mv~.,Hedonic.train),k=log(nrow(Hedonic.train)))

aic.anova<-step.AIC$anova
aic.anova

bic.anova<-step.BIC$anova
bic.anova

summary(step.AIC)

summary(step.BIC)

plot(step.AIC)


plot(step.BIC)


```

<p> En utilisant le crit√®re AIC, on trouve qu'il est plus pr√©cis d'utiliser 5 variables pour avoir le meilleur mod√®le, les variables "age", "indus", "zn", "townid" et "chas" tandis que l'utilisation du crit√®re BIC montre que le meilleur mod√®le est celui √† 6 variables en ajoutant la variable "blacks". </p>


<p> A pr√©sent, utilisons la validation crois√©e pour trouver une valeur de lambda. Nous utiliserons un fold de 10 puis un fold qui sera le nombre d'individus contenus dans le tableau. </p>
```{r echo=FALSE}
library(glmnet)



x <- as.matrix(Hedonic.train[,-1])
y <- Hedonic.train$mv

ridge.min<-cv.glmnet(x,y,alpha=0, nfolds=10)#nfolds validation croiseeleave one out a revoire
n <-nrow(Hedonic.train)
ridge.1se<-cv.glmnet(x,y,alpha=0,nfolds=n,grouped=FALSE)#pas sur
ridge.var<-glmnet(x,y,alpha=0)

#ridge.path<-
 
plot(ridge.1se)
plot(ridge.min)

```

<p> Le mod√®le du ridge calcul√© par la r√®gle du minimum et celui calcul√© par la r√®gle du "1 standard error" semblent quasiment identiques, la courbe d'√©volution de l'erreur en fonction de lambda √©tant identiques pour les deux. </p>

```{r echo=FALSE}

log(ridge.1se$lambda.min)
log(ridge.1se$lambda.1se)


lbs_fun <- function(ridge.var, ...) {
        L <- length(ridge.var$lambda)
        x <- log(ridge.var$lambda[L])
        y <- ridge.var$beta[, L]
        labs <- names(y)
        text(x, y, labels=labs, ...)
        legend('topright', legend=labs, col=1:length(labs), lty=1) #
}
plot(ridge.var, xvar="lambda", col=1:dim(coef(ridge.var))[1])
lbs_fun(ridge.var)

```

<p> En augmentant la valeur de Lambda pour r√©gulariser le mod√®le, nous pouvons voir que les param√®tres qui convergent le moins rapidement vers 0 sont les variables "chas", "lstat", "blacks", puis "dis". Ces param√®tres sont donc les meilleurs pr√©dicteurs du mod√®le. Les param√®tres "rad" et "indus" convergent √©galement mois rapidement que tout les autres param√®tres qui, eux, semblent convergent tr√®s rapidement vers 0 et sont donc les moins importants pour la pr√©diction et l'explication du param√®tre "mv". </p> 

```{r echo=FALSE}

plot(ridge.var,xvar="norm")
lbs_fun(ridge.var)
plot(ridge.var,xvar="dev")
lbs_fun(ridge.var)

```
<p> Les graphes utilisant comme terme de r√©gularisation la norme L1 et la fraction de d√©viance mettent en valeur la variable "rm" comme un param√®tre √©galement important pour la pr√©diction et l'explication de la variable "mv".</p>

```{r echo=FALSE}

x0 <-as.matrix(Hedonic.train[1:5, -1])
predict(ridge.var,newx=x0,s=ridge.min$lambda.min)
predict(ridge.var,newx=x0,s=ridge.min$lambda.1se)

```

<p> Voici ci-dessus les valeurs pr√©dites des Betas pour les deux valeurs de lambdas √©valu√©es. Nous pouvons voir √† nouveau que les valeurs des betas sont sensiblement identiques pour les deux mod√®les. Seul 5 valeurs de betas ont √©t√© √©valu√©es, celles correspondants aux variables que la regression ridge a jug√© les plus √† m√™me de servir de pr√©dicteurs (!!! dire lesquelles, demander au prof si ce que je pense c'est bon !!!) </p>


<p> A pr√©sent, nous utilisons la r√©gression Lasso pour avoir un choix de pr√©dicteurs plus significatif. </>

```{r echo=FALSE}

lasso <- glmnet(x,y, alpha=1)
lbs_fun <- function(lasso, ...) {
        L <- length(lasso$lambda)
        x <- log(lasso$lambda[L])
        y <- lasso$beta[, L]
        labs <- names(y)
        text(x, y, labels=labs, ...)
        legend('topright', legend=labs, col=1:length(labs), lty=1) #
}

plot(lasso, xvar="lambda")
lbs_fun(lasso)

```

<p> A pr√©sent, nous utilisons la m√©thode Lasso pour r√©gulariser le mod√®le. Tout d'abord, ce premier graphe sur lequel on voit l'√©volution des coefficients de chaque variable en fonction de Lambda nous permet de voir quels sont les variables qui semblent plus efficace pour servir de pr√©dicteur : les variables dont les coefficients sont rapidement r√©duits √† z√©ro sont les plus mauvais pr√©dicteurs. Ainsi les variables dont les coefficients sont r√©duits √† z√©ro avec une valeur de lambda plus importante que les autres sont les meilleurs pr√©dicteurs. Gr√¢ce √† ce graphe, nous pouvons donc observer que les variables "rad", "townid", "chas", "tax", "indus" et "nox" semblent √™tre les plus √† m√™me de pr√©dire le mod√®le. </p>

```{r echo=FALSE}
plot(lasso,xvar="norm")
lbs_fun(lasso)


```
<p> Ce second graphe nous am√®ne aux m√™mes conclusions que le dernier, √† la diff√©rence qu'ici le terme de r√©gularisation utilis√© est la norme L1. </p>

```{r echo=FALSE}
lbs_fun2 <- function(lasso, ...) {
        L <- length(lasso$lambda)
        x <- log(lasso$lambda[L])
        y <- lasso$beta[, L]
        labs <- names(y)
        text(x, y, labels=labs, ...)
        legend('topleft', legend=labs, col=1:length(labs), lty=1) #
}
plot(lasso,xvar="dev")
lbs_fun2(lasso)

```
<p> Le raisonnement et les conclusions sont identiques pour ce troisi√®me graphe. Ici, le terme de r√©gularisation utilis√© est la fraction de d√©viance </p>


```{r echo=FALSE}
lasso.min <-cv.glmnet(x,y,nfolds=10,grouped=FALSE)
lasso.1se  <-cv.glmnet(x,y,nfolds=n ,grouped=FALSE)
plot(lasso.min)
plot(lasso.1se)
```

```{r echo=FALSE}
predict(lasso, x0,s=lasso.min$lambda.min)
predict(lasso, x0,s=lasso.1se$lambda.1se)

```
<p>Commentaire</p>

```{r echo=FALSE}
library(ggplot2)
n <-nrow(Hedonic.train)
p <-ncol(Hedonic.train) 
AIC  <- n*log(colMeans((y -predict(lasso, x))^2)) + 2 * lasso$df
BIC  <- n*log(colMeans((y -predict(lasso, x))^2)) +log(n) * lasso$df
eBIC <- n*log(colMeans((y -predict(lasso, x))^2)) +log(p) * lasso$df
mBIC <- n*log(colMeans((y -predict(lasso, x))^2)) + (log(n) + 2 *log(p)) * lasso$df
d <-data.frame(lambda  =rep(lasso$lambda, 4),value   =c(AIC, BIC, eBIC, mBIC),critere =factor(rep(c("AIC","BIC","eBIC","mBIC"), each=length(lasso$lambda))))
ggplot(d,aes(x=lambda,y=value,colour=critere,group=critere)) +geom_line() +scale_x_log10()
lambda.min.BIC  <-lasso$lambda[which.min(BIC)]
lambda.min.mBIC <-lasso$lambda[which.min(mBIC)]
```
<p> commentaires </p>



#Quatri√®me partie: √©valuation de la qualit√© des mod√®les
```{r echo=FLASE}
Hedonic.test$chas<-as.numeric(Hedonic.test$chas)
y.test <-Hedonic.test$mv
x.test <-as.matrix(Hedonic.test[, -1])
err.null <-mean((y.test -predict(null, Hedonic.test))^2)
err.full <-mean((y.test -predict(full, Hedonic.test))^2)

err.sAIC <-mean((y.test -predict(step.AIC, Hedonic.test))^2)
err.sBIC <-mean((y.test -predict(step.BIC, Hedonic.test))^2)

err.ridge.min  <-mean((y.test -predict(ridge.var, newx=x.test, s=ridge.min$lambda.min))^2)

err.ridge.1se  <-mean((y.test -predict(ridge.var, newx=x.test, s=ridge.1se$lambda.min))^2)

err.lasso.min  <-mean((y.test -predict(lasso, newx=x.test, s=lasso.min$lambda.min))^2)

err.lasso.1se  <-mean((y.test -predict(lasso, newx=x.test, s=lasso.1se$lambda.min))^2)

err.lasso.BIC  <-mean((y.test -predict(lasso, newx=x.test, s=lambda.min.BIC))^2)

err.lasso.mBIC <-mean((y.test -predict(lasso,newx=x.test, s=lambda.min.mBIC))^2)
res <-data.frame(modele =c("null", "full", "step.AIC", "step.BIC", "ridge.CVmin","ridge.CV1se", "lasso.CVmin", "lasso.CV1se","lasso.BIC","lasso.mBIC"),erreur =c(err.null, err.full, err.sAIC, err.sBIC, err.ridge.min, err.ridge.1se,err.lasso.min, err.lasso.1se, err.lasso.BIC, err.lasso.mBIC))

print(res)
```

<p> Commentaires </p>

<p> utilisation de base polynomiale et de splines </p>
```{r}
install.packages("splines")
library(splines)

poly <- lm(mv ~ bs(crim) + bs(indus)  + bs(nox) + bs(rm) + bs(age) + bs(dis) + bs(rad)+ bs(tax) + bs(ptratio) + bs(blacks) + bs(lstat) , data = Hedonic.train )
summary(poly)
predict(poly, Hedonic.test )
x.test <-as.matrix(Hedonic.test[, -1])
err.poly <-mean((y.test -predict(poly, Hedonic.test))^2)
res2 <-data.frame(modele =c("null", "full", "step.AIC", "step.BIC", "ridge.CVmin","ridge.CV1se", "lasso.CVmin", "lasso.CV1se","lasso.BIC","lasso.mBIC", "poly"),erreur =c(err.null, err.full, err.sAIC, err.sBIC, err.ridge.min, err.ridge.1se,err.lasso.min, err.lasso.1se, err.lasso.BIC, err.lasso.mBIC, err.poly))
print(res2)
step(poly)
poly2 <- lm(mv ~  bs(indus)  + bs(age) + bs(rad)+ bs(tax)  + bs(blacks) + bs(dis) + bs(ptratio) + bs(nox) , data = Hedonic.train )
summary(poly2)
predict(poly2, Hedonic.test )

err.poly2 <-mean((y.test -predict(poly2, Hedonic.test))^2)
res3 <-data.frame(modele =c("null", "full", "step.AIC", "step.BIC", "ridge.CVmin","ridge.CV1se", "lasso.CVmin", "lasso.CV1se","lasso.BIC","lasso.mBIC", "poly2"),erreur =c(err.null, err.full, err.sAIC, err.sBIC, err.ridge.min, err.ridge.1se,err.lasso.min, err.lasso.1se, err.lasso.BIC, err.lasso.mBIC, err.poly2))
print(res3)

poly3 <- lm(mv ~  bs(indus*age)  +  bs(age*rad)  +  bs(rad*tax)+ bs(tax*blacks)  + bs(blacks*dis) + bs(dis*ptratio) + bs(ptratio*nox) + bs(nox)   , data = Hedonic.train )
Hedonic2 <- Hedonic.train
Hedonic2$chas <- NULL
Hedonic2$townid <- NULL
ncol(Hedonic2)
head(Hedonic.train)

for(i in 2:12){
  for(j in (i+1):13){
#    assign(paste(paste("var",1,sep="_"),2,sep="_"),(Hedonic2[,1]*Hedonic2[,2]))
#    var1
#    eval(paste("var",1,sep=""))
#      Hedonic2 <- cbind(Hedonic2,paste("var",1,sep=""))
    Hedonic2 <- cbind(Hedonic2,Hedonic2[,i]*Hedonic2[,j])
    }
}

colnames(Hedonic2) <- 1:ncol(Hedonic2)
ncol(Hedonic2)
colnames(Hedonic2)[1] <- "mv"

step.AIC <-step(lm(mv~.,Hedonic2),k=2, direction="both")

step.BIC <-step(lm(mv~.,Hedonic2),k=log(nrow(Hedonic2)))

aic.anova<-step.AIC$anova
aic.anova

bic.anova<-step.BIC$anova
bic.anova

summary(step.AIC)
Hedonic.train<-Hedonic2
head(Hedonic2)
ncol(Hedonic2)
summary(poly3)
predict(poly3, Hedonic.test )

err.poly3 <-mean((y.test -predict(poly3, Hedonic.test))^2)
res4 <-data.frame(modele =c("null", "full", "step.AIC", "step.BIC", "ridge.CVmin","ridge.CV1se", "lasso.CVmin", "lasso.CV1se","lasso.BIC","lasso.mBIC", "poly3"),erreur =c(err.null, err.full, err.sAIC, err.sBIC, err.ridge.min, err.ridge.1se,err.lasso.min, err.lasso.1se, err.lasso.BIC, err.lasso.mBIC, err.poly3))
print(res4)


```

<p> On remarque qu'on a une plus grande erreur dans le modËle avec les polynomes que pour les autres modËles dÈfinit auparavant </p>